{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T08:43:17.316721Z",
     "iopub.status.busy": "2023-05-23T08:43:17.316135Z",
     "iopub.status.idle": "2023-05-23T08:43:27.549142Z",
     "shell.execute_reply": "2023-05-23T08:43:27.548141Z"
    },
    "id": "TtcwSIcgbIVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q imageio\n",
    "!pip install -q opencv-python\n",
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T08:43:27.553221Z",
     "iopub.status.busy": "2023-05-23T08:43:27.552980Z",
     "iopub.status.idle": "2023-05-23T08:43:30.128968Z",
     "shell.execute_reply": "2023-05-23T08:43:30.128152Z"
    },
    "id": "9BLeJv-pCCld"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-05-23T08:43:30.133649Z",
     "iopub.status.busy": "2023-05-23T08:43:30.133229Z",
     "iopub.status.idle": "2023-05-23T08:43:30.151181Z",
     "shell.execute_reply": "2023-05-23T08:43:30.150635Z"
    },
    "id": "bEJBMeRb3YUy"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, duration):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UvrN0iQiOxhR"
   },
   "source": [
    "## Load Model from TF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T08:43:30.154504Z",
     "iopub.status.busy": "2023-05-23T08:43:30.153990Z",
     "iopub.status.idle": "2023-05-23T08:43:42.641673Z",
     "shell.execute_reply": "2023-05-23T08:43:42.640950Z"
    },
    "id": "zeGHgANcT7a1"
   },
   "outputs": [],
   "source": [
    "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the camera capture\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the capture frame width and height\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def innner_P(ax, ay, bx, by, cx, cy):\n",
    "    num_innnerP = (ax - bx)*(cx - bx) + (ay - by) * (cy - by)\n",
    "    return num_innnerP\n",
    "\n",
    "def change_into_angle(ax, ay, bx, by, cx, cy, num_innnerP):\n",
    "    size_num = math.sqrt((ax - bx) ** 2 + (ay - by) ** 2) * math.sqrt((cx - bx) ** 2 + (cy - by) ** 2)\n",
    "    \n",
    "    # Check if size_num is zero\n",
    "    if size_num == 0:\n",
    "        return 0\n",
    "    \n",
    "    angle_radians = math.acos(num_innnerP / size_num)\n",
    "    angle_degrees = math.degrees(angle_radians)\n",
    "    return angle_degrees\n",
    "\n",
    "def calculate_average_angle(keypoints_with_scores):\n",
    "    right_arm_angles = []\n",
    "    left_arm_angles = []\n",
    "\n",
    "    keypoints = keypoints_with_scores[0][0]  # Access the keypoints array\n",
    "    \n",
    "    # Variables to store x and y values of keypoints\n",
    "    x8, y8 = keypoints[8][:2]\n",
    "    x6, y6 = keypoints[6][:2]\n",
    "    x12, y12 = keypoints[12][:2]\n",
    "    x7, y7 = keypoints[7][:2]\n",
    "    x5, y5 = keypoints[5][:2]\n",
    "    x11, y11 = keypoints[11][:2]\n",
    "    \n",
    "    right_innner_P = innner_P(x8, y8, x6, y6, x12, y12)\n",
    "    right_angle = change_into_angle(x8, y8, x6, y6, x12, y12, right_innner_P)\n",
    "    left_innner_P = innner_P(x7, y7, x5, y5, x11, y11)\n",
    "    left_angle = change_into_angle(x7, y7, x5, y5, x11, y11, left_innner_P)\n",
    "    \n",
    "    return (right_angle + left_angle) / 2.0  # Remove the trailing dot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result(angle):\n",
    "    if 0 <= angle <= 20:\n",
    "        return -80\n",
    "    elif 20 < angle <= 120:\n",
    "        return 0\n",
    "    elif 120 < angle <= 180:\n",
    "        return 80\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "address = '10.63.60.215'  # 宛先のIPアドレス\n",
    "port = 8080  # 宛先のポート番号\n",
    "\n",
    "def send_message(address, port, message):\n",
    "    # ソケットを作成してデータを送信する\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.connect((address, port))  # サーバに接続\n",
    "        encoded_message = message.encode('utf-8')  # メッセージをUTF-8エンコード\n",
    "        s.sendall(encoded_message)  # メッセージを送信"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture and process frames\n",
    "def process_frames():\n",
    "    # Capture frames every 1 second\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize and pad the frame to fit the expected size\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_image = tf.expand_dims(image, axis=0)\n",
    "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "        # Run model inference\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "        # Visualize the predictions with frame\n",
    "        display_frame = cv2.resize(image, (1280, 1280))\n",
    "        output_overlay = draw_prediction_on_image(display_frame, keypoints_with_scores)\n",
    "\n",
    "        # Display the frame with keypoints\n",
    "        cv2.imshow(\"Output\", output_overlay)\n",
    "        keypoints_with_scores_np = np.array(keypoints_with_scores)\n",
    "        # 2. Numpy配列をテキスト形式に変換\n",
    "        keypoints_text = np.array2string(keypoints_with_scores_np, separator=', ')\n",
    "        angle = calculate_average_angle(keypoints_with_scores_np)\n",
    "        # angle = convert_angle(angle)\n",
    "        send_message(address, port, str(angle))\n",
    "        print(angle)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(10) == ord('q'):  # Capture for 1 second (1000 milliseconds)\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ずっと値を送り続けるもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "80\n",
      "80\n",
      "80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "80\n",
      "80\n",
      "80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "0\n",
      "80\n",
      "80\n",
      "0\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "0\n",
      "0\n",
      "80\n",
      "0\n",
      "80\n",
      "0\n",
      "80\n",
      "80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "80\n",
      "0\n",
      "0\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-80\n",
      "-80\n",
      "-80\n",
      "-80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# 1秒の遅延\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     process_frames()\n",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mprocess_frames\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     20\u001b[0m input_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m input_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mresize_with_pad(input_image, input_size, input_size)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Run model inference\u001b[39;00m\n\u001b[1;32m     24\u001b[0m keypoints_with_scores \u001b[39m=\u001b[39m movenet(input_image)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/ops/image_ops_impl.py:1935\u001b[0m, in \u001b[0;36mresize_image_with_pad_v2\u001b[0;34m(image, target_height, target_width, method, antialias)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resize_fn\u001b[39m(im, new_size):\n\u001b[1;32m   1933\u001b[0m   \u001b[39mreturn\u001b[39;00m resize_images_v2(im, new_size, method, antialias\u001b[39m=\u001b[39mantialias)\n\u001b[0;32m-> 1935\u001b[0m \u001b[39mreturn\u001b[39;00m _resize_image_with_pad_common(image, target_height, target_width,\n\u001b[1;32m   1936\u001b[0m                                      _resize_fn)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/ops/image_ops_impl.py:1816\u001b[0m, in \u001b[0;36m_resize_image_with_pad_common\u001b[0;34m(image, target_height, target_width, resize_fn)\u001b[0m\n\u001b[1;32m   1813\u001b[0m _, height, width, _ \u001b[39m=\u001b[39m _ImageDimensions(image, rank\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m   1815\u001b[0m \u001b[39m# convert values to float, to ease divisions\u001b[39;00m\n\u001b[0;32m-> 1816\u001b[0m f_height \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39;49mcast(height, dtype\u001b[39m=\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m   1817\u001b[0m f_width \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mcast(width, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m   1818\u001b[0m f_target_height \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mcast(target_height, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1009\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1003\u001b[0m   x \u001b[39m=\u001b[39m indexed_slices\u001b[39m.\u001b[39mIndexedSlices(values_cast, x\u001b[39m.\u001b[39mindices, x\u001b[39m.\u001b[39mdense_shape)\n\u001b[1;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1005\u001b[0m   \u001b[39m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m   \u001b[39m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m   \u001b[39m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m   \u001b[39m# strings.\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m   x \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(x, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1010\u001b[0m   \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex \u001b[39mand\u001b[39;00m base_type\u001b[39m.\u001b[39mis_floating:\n\u001b[1;32m   1011\u001b[0m     logging\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1012\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are casting an input of type \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m to an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mincompatible dtype \u001b[39m\u001b[39m{\u001b[39;00mbase_type\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.  This will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1014\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdiscard the imaginary part and may not be what you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1015\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mintended.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1615\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1611\u001b[0m   preferred_dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(preferred_dtype)\n\u001b[1;32m   1613\u001b[0m \u001b[39m# See below for the reason why it's `type(value)` and not just `value`.\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m \u001b[39m# https://docs.python.org/3.8/reference/datamodel.html#special-lookup\u001b[39;00m\n\u001b[0;32m-> 1615\u001b[0m overload \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(value), \u001b[39m\"\u001b[39;49m\u001b[39m__tf_tensor__\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1616\u001b[0m \u001b[39mif\u001b[39;00m overload \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1617\u001b[0m   \u001b[39mreturn\u001b[39;00m overload(value, dtype, name)  \u001b[39m#  pylint: disable=not-callable\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prev_input = 0  # Initialize prev_input outside the process_frames() function\n",
    "\n",
    "def process_frames():\n",
    "    # Connect to the server\n",
    "    # address = 'localhost'\n",
    "    # port = 8081\n",
    "    # send_message(address, port)\n",
    "\n",
    "    while True:\n",
    "        # Capture and send frames continuously\n",
    "        while True:\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Resize and pad the frame to fit the expected size\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_image = tf.expand_dims(image, axis=0)\n",
    "            input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "            # Run model inference\n",
    "            keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "            # Visualize the predictions with frame\n",
    "            display_frame = cv2.resize(image, (1280, 1280))\n",
    "            output_overlay = draw_prediction_on_image(display_frame, keypoints_with_scores)\n",
    "\n",
    "            # Display the frame with keypoints\n",
    "            cv2.imshow(\"Output\", output_overlay)\n",
    "            keypoints_with_scores_np = np.array(keypoints_with_scores)\n",
    "            # 2. Numpy配列をテキスト形式に変換\n",
    "            keypoints_text = np.array2string(keypoints_with_scores_np, separator=', ')\n",
    "            angle = calculate_average_angle(keypoints_with_scores_np)\n",
    "            # send_message(address, port, str(angle))\n",
    "            result = calculate_result(angle)\n",
    "            print(result)\n",
    "            global prev_input\n",
    "            if result != prev_input:\n",
    "                prev_input = result\n",
    "                send_message(address, port, str(result))\n",
    "            \n",
    "            if cv2.waitKey(10) == ord('q'):  # Capture for 1 second (1000 milliseconds)\n",
    "                break\n",
    "            time.sleep(0.1)  # 0.1秒の遅延\n",
    "\n",
    "        # Release the capture and close windows\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Reconnect to the server\n",
    "        # send_message(address, port, 'RECONNECT')\n",
    "        time.sleep(1)  # 1秒の遅延\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_frames()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9u_VGR6_BmbZ",
    "5I3xBq80E3N_",
    "L2JmA1xAEntQ"
   ],
   "name": "MoveNet_SinglePose_Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
